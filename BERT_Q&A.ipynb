{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591,
          "referenced_widgets": [
            "0759f0917df3491d812f2a29dd4f2c55",
            "d44ac0eb1c8f48488426fa7a45e0ef5c",
            "bb2b3ca519444ab68876600f21f75a14",
            "8f57820f79a44d52b528b9c680863739",
            "343f210ef1924d74acc5c32fb5bdea24",
            "39c52bb18ff341d6b574af0c47248202",
            "fdc47f6f528c470e9a1dbeb2cf64785e",
            "86e61cfcd5194bd48b863d5548ae6b0a",
            "793a1b11d03144ac98277501131755f4",
            "2c8cd9fcb34a4539a7a4f26333609611",
            "45632da963a14904b5574bbe71717a7f",
            "9abf43c725844441b61dc7df9c9ff3a9",
            "db2cc76beedb48209fd099f66db9d800",
            "6fb99931547f4a93959c044f222b08da",
            "082cdccf30aa4d07b1f603f4c6e82c4a",
            "90c3a4d7e16f4227881b89a446dc7c99",
            "f120a79a0376461dac63037e363535fe",
            "a0fcce3aa2914bdf8b9d70314eec133d",
            "1fe186130c254dc5a91e18902f5b3834",
            "34b436e4627c4bc19f0d2d84dc0ee786",
            "10318529fff149949af79e8a28ffd644",
            "703fabb63127418d9b4e003d8e39c879"
          ]
        },
        "id": "2rz-7i2MBT7G",
        "outputId": "ab190f38-e974-46fa-8a40-59e1ccb705eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Dataset loaded successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train data:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0759f0917df3491d812f2a29dd4f2c55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing validation data:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9abf43c725844441b61dc7df9c9ff3a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Tokenization complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-403978209.py:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\ude80 Training started...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [770/770 02:41, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.210000</td>\n",
              "      <td>1.622930</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Training completed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udcc8 Evaluation Results: {'eval_loss': 1.6030237674713135, 'eval_runtime': 3.0564, 'eval_samples_per_second': 170.137, 'eval_steps_per_second': 21.267, 'epoch': 2.0}\n",
            "\u2705 Approximate F1-like score: 0.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2705 Fine-tuned model and tokenizer saved to './fine_tuned_bert_qa'\n",
            "\ud83d\udcac Interactive Q&A System Ready!\n",
            "Type 'quit' anytime to stop.\n",
            "\n",
            "Enter context (or 'quit' to exit): The Apollo program was the third United States human spaceflight program carried out by NASA, which accomplished landing the first humans on the Moon from 1969 to 1972. It was first conceived during Dwight D. Eisenhower's administration as a three-person spacecraft to follow the one-person Project Mercury.\n",
            "Enter your question: Who carried out the Apollo program?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer: NASA\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast,BertForQuestionAnswering,TrainingArguments,Trainer, pipeline\n",
        "df = load_dataset(\"SQuAD\")\n",
        "train_subset = df[\"train\"].select(range(3000))\n",
        "val_subset = df[\"validation\"].select(range(500))\n",
        "print(\"\u2705 Dataset loaded successfully\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    contexts = examples[\"context\"]\n",
        "\n",
        "    tokenized_examples = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        truncation=\"only_second\",\n",
        "        max_length=384,\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        sample_idx = sample_mapping[i]\n",
        "        answer = examples[\"answers\"][sample_idx]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answer[\"text\"][0])\n",
        "        context_index = 1\n",
        "\n",
        "        token_start = None\n",
        "        token_end = None\n",
        "\n",
        "        for idx, (seq_id, (start, end)) in enumerate(zip(sequence_ids, offsets)):\n",
        "            if seq_id == context_index:\n",
        "                if start <= start_char < end:\n",
        "                    token_start = idx\n",
        "                if start < end_char <= end:\n",
        "                    token_end = idx\n",
        "                    break\n",
        "\n",
        "        if token_start is None or token_end is None:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "        else:\n",
        "            start_positions.append(token_start)\n",
        "            end_positions.append(token_end)\n",
        "\n",
        "    tokenized_examples[\"start_positions\"] = start_positions\n",
        "    tokenized_examples[\"end_positions\"] = end_positions\n",
        "    return tokenized_examples\n",
        "\n",
        "tokenized_train = train_subset.map(preprocess_function, batched=True, remove_columns=train_subset.column_names, desc=\"Tokenizing train data\")\n",
        "tokenized_valid = val_subset.map(preprocess_function, batched=True, remove_columns=val_subset.column_names, desc=\"Tokenizing validation data\")\n",
        "print(\"\u2705 Tokenization complete\")\n",
        "\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "from accelerate import Accelerator\n",
        "Accelerator().free_memory()\n",
        "\n",
        "trainer.train()\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"\ud83d\udcc8 Evaluation Results:\", eval_results)\n",
        "\n",
        "predictions = trainer.predict(tokenized_valid)\n",
        "start_logits, end_logits = predictions.predictions\n",
        "start_preds = np.argmax(start_logits, axis=1)\n",
        "end_preds = np.argmax(end_logits, axis=1)\n",
        "f1_like_score = np.mean(start_preds == end_preds)\n",
        "print(f\"\u2705 Approximate F1-like score: {f1_like_score:.2f}\")\n",
        "\n",
        "model.save_pretrained(\"./fine_tuned_bert_qa\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bert_qa\")\n",
        "print(\"\u2705 Fine-tuned model and tokenizer saved to './fine_tuned_bert_qa'\")\n",
        "\n",
        "\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"./fine_tuned_bert_qa\",\n",
        "    tokenizer=\"./fine_tuned_bert_qa\"\n",
        ")\n",
        "\n",
        "print(\"\ud83d\udcac Interactive Q&A System Ready!\")\n",
        "print(\"Type 'quit' anytime to stop.\\n\")\n",
        "\n",
        "while True:\n",
        "    context = input(\"Enter context (or 'quit' to exit): \")\n",
        "    if context.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    question = input(\"Enter your question: \")\n",
        "    if question.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    result = qa_pipeline({\n",
        "        \"context\": context,\n",
        "        \"question\": question\n",
        "    })\n",
        "\n",
        "    print(f\"\\nAnswer: {result['answer']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n"
      ]
    }
  ]
}